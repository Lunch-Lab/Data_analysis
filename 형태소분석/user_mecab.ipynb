{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","mount_file_id":"19qghjo4X6jQrnk7HXOLaEHhV71j2dGXj","authorship_tag":"ABX9TyMz5ZgGfrTskaTgrsv1CozX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# User 리뷰 형태소 분석\n","## 1. Colab에 Mecab 설치"],"metadata":{"id":"Yjowvts8BUcu"}},{"cell_type":"code","source":[],"metadata":{"id":"WUy_cN9sR_uJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive # 구글 드라이브 마운트 작업\n","drive.mount('/content/drive')"],"metadata":{"id":"CTGRXdsUgtsz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hemYSOryBQwv"},"outputs":[],"source":["# Colab에 Mecab 설치\n","!pip install konlpy\n","!pip install mecab-python\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n"]},{"cell_type":"markdown","source":["## 2.유저 리뷰 데이터 전처리"],"metadata":{"id":"k1DJvOtyBcsb"}},{"cell_type":"code","source":["import re\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import urllib.request\n","from collections import Counter\n","from konlpy.tag import Mecab\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"2K6oAZ7WBaKt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G26WL9BwB0Zj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1) 데이터 로드\n"],"metadata":{"id":"WpH95wdBB1tW"}},{"cell_type":"code","source":["# import warnings\n","# import pandas as pd\n","# import chardet\n","\n","# warnings.simplefilter(action='ignore', category=pd.errors.ParserWarning)\n","\n","\n","\n","# # 감지된 인코딩을 사용하여 파일을 읽습니다\n","# total_data = pd.read_csv(\"/content/통합유저.csv\", encoding='latin1', on_bad_lines='skip')\n"],"metadata":{"id":"F_jjR391B45Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","total_data = pd.read_excel('/content/drive/MyDrive/통합유저_전처리완.xlsx')\n"],"metadata":{"id":"JNUlOZguZvMD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"slOHFuq_hHd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_data[:5]"],"metadata":{"id":"L0f7cZxqB9gR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2) 훈련 데이터 & 테스트 데이터 분리"],"metadata":{"id":"UOCF15GbCBJ9"}},{"cell_type":"code","source":[],"metadata":{"id":"KV8eTMycB_8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_data[:5]"],"metadata":{"id":"bayM8yM6DKZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 중복 제거\n","\n","total_data.drop_duplicates(subset=['place_review'],inplace=True)\n","\n","print('총 샘플의 수 :',len(total_data))"],"metadata":{"id":"VavgJSwPDWhJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(total_data.isnull().values.any())"],"metadata":{"id":"kIIw0ImaDmLj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 훈련 데이터와 테스트 데이터를 3:1 비율로 분리\n","from sklearn.model_selection import train_test_split\n","train_data , test_data = train_test_split(total_data,test_size=0.25,random_state =43)\n","\n","print('훈련용 리뷰의 개수 :', len(train_data))\n","print('테스트용 리뷰의 개수 :', len(test_data))"],"metadata":{"id":"EVqsmXg-DuAs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3) 데이터 정제"],"metadata":{"id":"aA4psnhsEYC6"}},{"cell_type":"code","source":["# 한글과 공백을 제외하고 모두 제거\n","import numpy as np\n","train_data['place_review'] = train_data['place_review'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n","train_data['place_review'].replace('', np.nan, inplace=True)\n","print(train_data.isnull().sum())"],"metadata":{"id":"a-QCIZgQEVk9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data.drop_duplicates(subset = ['place_review'], inplace=True) # 중복 제거\n","test_data['place_review'] = test_data['place_review'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n","test_data['place_review'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n","test_data = test_data.dropna(how='any') # Null 값 제거\n","print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"],"metadata":{"id":"j3BQfinmEfsk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5) 토큰화\n","\n","형태소 분석기 Mecab을 사용하여 토큰화 작업을 수행합니다. 다음은 임의의 문장에 대해서 테스트한 토큰화 결과입니다."],"metadata":{"id":"zGzibkkCEjnZ"}},{"cell_type":"code","source":["# Colab에 Mecab 설치\n","!pip install konlpy\n","!pip install mecab-python\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n"],"metadata":{"id":"R2lvH63qBy_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import urllib.request\n","from collections import Counter\n","from konlpy.tag import Mecab\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"EknV62iFBzlC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mecab = Mecab()\n","print(mecab.morphs('와 이런 것도 상품이라고 차라리 내가 만드는 게 나을 뻔'))"],"metadata":{"id":"nJmJGL9BEhZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 불용어 지정해서 필요없는 토큰 제거\n","\n","stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']"],"metadata":{"id":"BdmU36ThEoBb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data['tokenized'] = train_data['place_review'].apply(mecab.morphs)\n","train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n","\n"],"metadata":{"id":"T7pf2BYbEt8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data['tokenized'] = test_data['place_review'].apply(mecab.morphs)\n","test_data['tokenized'] = test_data['tokenized'].apply(lambda x : [item for item in x if item not in stopwords])"],"metadata":{"id":"WXkLRdDbFBhg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6) 단어와 길이 분포 확인\n","긍정 리뷰에는 주로 어떤 단어들이 많이 등장하고, 부정 리뷰에는 주로 어떤 단어들이 등장하는지 두 가지 경우에 대해서 각 단어의 빈도수를 계산해보겠습니다. 각 레이블에 따라서 별도로 단어들의 리스트를 저장해줍니다."],"metadata":{"id":"x0qzeOxxFbHl"}},{"cell_type":"code","source":["# 긍정과 부정리뷰 구분\n","# np.hstack() 함수는 주어진 배열들을 수평(가로)으로 합치는 기능\n","\n","# negative_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n","# positive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)"],"metadata":{"id":"p63wurpJFYCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Counter()를 사용하여 각 단어에 대한 빈도수를 카운트\n","\n","# negative_word_count = Counter(negative_words)\n","# print(negative_word_count.most_common(22))"],"metadata":{"id":"tM6ZvIRsFi_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# positive_word_count = Counter(positive_words)\n","# print(positive_word_count.most_common(20))"],"metadata":{"id":"WmzJ55KwF_ML"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n","# text_length = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))\n","# ax1.hist(text_length, color='red')\n","# ax1.set_title('Positive Reviews')\n","# ax1.set_xlabel('length of samples')\n","# ax1.set_ylabel('number of samples')\n","# print('긍정 리뷰의 평균 길이 :', np.mean(text_length))\n","\n","# text_length = train_data[train_data['label']==0]['tokenized'].map(lambda x : len(x))\n","# ax2.hist(text_length, color='blue')\n","# ax2.set_title('Negative Reviews')\n","# fig.suptitle('Words in texts')\n","# ax2.set_xlabel('length of samples')\n","# ax2.set_ylabel('number of samples')\n","# print('부정 리뷰의 평균 길이 :', np.mean(text_length))\n","# plt.show()"],"metadata":{"id":"FIQjB33nGlPS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = train_data['tokenized'].values\n","# y_train = train_data['label'].values\n","X_test= test_data['tokenized'].values\n","# y_test = test_data['label'].values"],"metadata":{"id":"M5VnY2TGHeT9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train"],"metadata":{"id":"QrSNpSZSDABa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7) 정수 인코딩\n","\n","기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행"],"metadata":{"id":"Z2KpVrX9IAdv"}},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train)"],"metadata":{"id":"x3RagwfXHj84"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 각 단어에 고유한 정수 부여 -> tokenizer.word_index 를 통해 확인 가능\n","\n","# 1번 등장하는 단어는 배제\n","\n","threshold = 2\n","total_cnt = len(tokenizer.word_index) # 단어수\n","rare_cnt = 0 # 등장 빈도수가 threshold 보다 작은 단어의 개수를 카운트\n","total_freq = 0  # 훈련 데이터의 전체 단어 빈도수 총 함\n","rare_freq = 0 # 등장 빈도수가 threshold 보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍을 key 와 value 로 받는다.\n","# 단어 : 빈도 로 나옴\n","for key,value in tokenizer.word_counts.items() :\n","    total_freq = total_freq + value\n","    # 단어의 등장 빈도수가 threshold보다 작으면 :\n","    if value < threshold :\n","        rare_cnt = rare_cnt +1\n","        rare_freq = rare_freq + value\n","\n","print('단어 집합(vocabulary)의 크기 :',total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"],"metadata":{"id":"5PBoL_ZcIGLw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["단어는 28393개 등장. 등장 빈도가 1번인\n","\n","단어는 전체에서 40% 차지.\n","\n","실제 훈련 데이터에서 회귀 단어의 등장 비율은 약 0.8% --> 자연어 처리에서 중요하지 않을것으로 판단. 제외시힘"],"metadata":{"id":"Pia-_dmzJWdA"}},{"cell_type":"code","source":["# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n","\n","# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n","\n","vocab_size = total_cnt - rare_cnt + 2\n","print('단어 집합의 크기 :',vocab_size)"],"metadata":{"id":"xD0pzA-dJUH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\n","tokenizer.fit_on_texts(X_train)\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)\n","\n","# oov_token: 사전에 없는 단어(Out-Of-Vocabulary, OOV)가 등장할 때 대체할 토큰을 지정합니다. oov_token='OOV'로 설정하면, 사전에 없는 단어는 모두 'OOV'로 대체됩니다.\n","# 이렇게 하면 모델이 예측할 때 사전에 없는 단어를 처리할 수 있습니다."],"metadata":{"id":"liqkunM0JzTD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train[:3])"],"metadata":{"id":"Ytr49tjdLmA9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_test[:3])"],"metadata":{"id":"SnoEPEpiLtID"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8) 패딩\n","서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 패딩"],"metadata":{"id":"Kdf525CVLv2d"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"zL0zAoU-D1OQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n","print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n","plt.hist([len(review) for review in X_train], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"],"metadata":{"id":"HvM-hKNLLub_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["리뷰의 최대 길이는 168, 평균 길이는 약 31"],"metadata":{"id":"wBPaMyIaL7YB"}},{"cell_type":"code","source":["def below_threshold_len(max_len,nested_list):\n","    count = 0\n","    for sentence in nested_list :\n","        if len(sentence) <= max_len:\n","            count = count+ 1\n","    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"],"metadata":{"id":"Jdm1yegeL0Pk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 168\n","below_threshold_len(max_len, X_train)"],"metadata":{"id":"eDyRcSKdMRi7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = pad_sequences(X_train, maxlen=max_len)\n","X_test = pad_sequences(X_test, maxlen=max_len)"],"metadata":{"id":"eue-bCQRMT2P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. GRU로 네이버 쇼핑 리뷰 감성 분류\n","\n","### GRU\n","\n","시계열 데이터나 순차 데이터를 다루기 위해 설계\n","\n","- GRu의 구조는 업데이트 게이트와 리셋 게이트\n","\n","    1) 업데이트 게이트는 이전 상태를 얼마나 유지하고 새로운 입력을 얼마나 반영할지 결정\n","\n","    2) 리셋 게이트는 이전 상태를 얼마나 무시할 지 결정\n","\n","    3) 새로운 메모리 내용\n","        - 리셋 게이트의 결과를 사용해 새로운 후보 활성 상태를 계산\n","    4) 최종 활성 상태\n","        - 업데이트 게이트와 새로운 메모리 내용을 사용해서 최종 상태를 계\n","\n","- GRU의 특징\n","\n","    1) 간단한 구조: LSTM보다 게이트 수가 적고, 계산 복잡도가 낮아 학습과 예측이 더 빠릅니다.\n","    \n","    2) 효율적인 학습: 적은 수의 게이트로도 장기 의존성을 효과적으로 학습할 수 있습니다.\n","    \n","    3) 성능: 많은 경우에서 LSTM과 유사한 성능을 보이며, 특히 데이터 양이 적거나 모델이 간단해야 할 때 유리합니다.\n","\n","하이퍼파라미터인 임베팅 벡터의 차원은 100, 은닉층은 128.\n","\n","모델은 다대일 구조의 LSTM을 사용 -> 두 개의 선택지 중 하나를 예측하는 이진분류 수행\n","\n","이진분류의 경우에는 활성화함수는 시그모이드, 손실함수는 크로스 엔트로피 함수 사용\n","\n","하이퍼파라미터인 배치크기는 64, 15에포크 수행\n","\n","EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n","\n","-> 검증 데이터 손실이 증가하면 과적합 징후이므로 검증 데이터손실이 4회 증가하면 학습을 조기종료\n","\n","ModelCheckpoint : 검증데이터의 정확도가 이전보다 좋아질 경우에만 모델을 저장\n","\n","validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고, 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인\n","\n"],"metadata":{"id":"7ZT21-3TMqXB"}},{"cell_type":"markdown","source":["1. 입력 처리\n","\n","    - 텍스트 데이터를 단어 또는 토큰 단위로 나누어 시퀀스로 변환합니다.\n","    - 각 단어를 임베딩 벡터로 변환하여 GRU의 입력으로 사용합니다.\n","\n","2. GRU 레이어\n","\n","    - GRU는 입력 시퀀스를 순차적으로 처리하며, 각 시점에서 새로운 입력과 이전 상태를 기반으로 새로운 후보 활성 상태를 계산합니다.\n","    - 이 활성 상태는 GRU 내부의 정보 저장소로, 문맥 정보를 담고 있습니다.\n","\n","3. 새로운 후보 활성 상태\n","\n","    - 새로운 후보 활성 상태 는 현재 시점에서 입력된 단어와 이전 시점의 정보를 조합하여 계산됩니다.\n","    \n","    - 이 활성 상태는 단순히 하나의 단어가 긍정인지 부정인지를 의미하지 않습니다. 대신, 현재까지 입력된 모든 단어의 정보를 종합한 문맥 정보를 나타냅니다.\n","4. 최종 활성 상태\n","\n","    - 각 시점의 활성 상태를 업데이트한 후, 마지막 시점의 활성 상태는 전체 문장의 문맥을 반영합니다.\n","    - 이 최종 상태를 사용하여 문장의 감성을 예측합니다.\n","5. 출력 레이어\n","\n","    - 최종 활성 상태를 사용하여 감성(긍정 또는 부정)을 예측합니다. 일반적으로, 이 과정은 하나 이상의 밀집층(Dense Layer)과 소프트맥스(Softmax) 또는 시그모이드(Sigmoid) 활성 함수로 이루어집니다."],"metadata":{"id":"nLJfcpxPPzYt"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Embedding, Dense, GRU\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","embedding_dim = 100\n","hidden_units = 128\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size,embedding_dim))\n","model.add(GRU(hidden_units))\n","model.add(Dense(1,activation = 'sigmoid'))\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n","\n","mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n","\n","model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n","\n","history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"],"metadata":{"id":"zVg-YA8iMWHF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model = load_model('best_model.h5')\n","print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"],"metadata":{"id":"5jtt24qNOhZ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"loGiau5pRCU6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. 리뷰 예측"],"metadata":{"id":"geD7mcK-RD0o"}},{"cell_type":"code","source":["def sentiment_predict(new_sentence):\n","  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n","  new_sentence = mecab.morphs(new_sentence)\n","  new_sentence = [word for word in new_sentence if not word in stopwords]\n","  encoded = tokenizer.texts_to_sequences([new_sentence])\n","  pad_new = pad_sequences(encoded, maxlen = max_len)\n","\n","  score = float(loaded_model.predict(pad_new))\n","  if(score > 0.5):\n","    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\n","  else:\n","    print(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))"],"metadata":{"id":"HUN99LoFRDsa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_predict('이 상품 진짜 좋아요... 저는 강추합니다. 대박')"],"metadata":{"id":"puVbvJtTRDoD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_predict('진짜 배송도 늦고 개짜증나네요. 뭐 이런 걸 상품이라고 만듬?')"],"metadata":{"id":"p_NXAuYORDlA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_predict('판매자님... 너무 짱이에요.. 대박나삼')"],"metadata":{"id":"2CcOBHM5RDii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_predict('ㅁㄴㅇㄻㄴㅇㄻㄴㅇ리뷰쓰기도 귀찮아')"],"metadata":{"id":"NwttQEXcRREo"},"execution_count":null,"outputs":[]}]}